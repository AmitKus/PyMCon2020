---
layout: post
title: 'Development environments in docker'
author: Bob
categories: [Docker]
image: assets/images/docker.png
---

This is a collection of information from various resources on the web and stackoverflow that is aimed to better understand the docker utility to serve as personal reference. We begin with the basic linux utility [chroot](https://www.cyberciti.biz/faq/unix-linux-chroot-command-examples-usage-syntax/), discuss how docker does more, define the lingo commonly used in docker and finally go through an example dockerfile to create ubuntu based linux dev environment. The dev env will check out a linux image, create a user with sudo privileges, install git features, install a popular shell (ohmyzsh) and finally we show how to mount a volume for persistent storage that is accessible from the containers. Remember that the default storage from a container is ephimeral.

## chroot

chroot command in Linux/Unix system is used to change the root directory. Every process/command in Linux/Unix like systems has a current working directory called root directory. It changes the root directory for currently running processes as well as its child processes. A process/command that runs in such a modified environment cannot access files outside the root directory. This modified environment is known as “chroot jail” or “jailed directory”.

![Pictorial description](https://media.geeksforgeeks.org/wp-content/uploads/chroot-command.jpg)

A chroot on Unix operating systems is an operation that changes the apparent root directory for the current running process and its children. A program that is run in such a modified environment cannot name (and therefore normally cannot access) files outside the designated directory tree. The modified environment is called a chroot jail.

### chroot command examples

In this example, build a mini-jail for testing purpose with bash and ls command only. First, set jail location using mkdir command:

```console
$ J=$HOME/jail
```

Create directories inside \$J:

```console
$ mkdir -p $J
$ mkdir -p $J/{bin,lib64,lib}
$ cd $J
```

Copy /bin/bash and /bin/ls into \$J/bin/ location using cp command:

```console
$ cp -v /bin/{bash,ls} $J/bin
```

Copy required libs in \$J. Use ldd command to print shared library dependencies for bash:

```console
$ ldd /bin/bash
```

Copy libs in \$J correctly from the above output:

```console
$ cp -v /lib64/libtinfo.so.5 /lib64/libdl.so.2 /lib64/libc.so.6 /lib64/ld-linux-x86-64.so.2 $J/lib64/
```

Copy required libs in \$J for ls command. Use ldd command to print shared library dependencies for ls command:

```console
$ ldd /bin/ls
```

Finally, chroot into your new jail:

```console
$ sudo chroot $J /bin/bash
```

A chrooted bash and ls application is locked into a particular directory called $HOME/$J and unable to wander around the rest of the directory tree, and sees that directory as its “/” (root) directory. This is a tremendous boost to security if configured properly.

## docker vs chroot

Docker allows to isolate a process at multiple levels through namespaces:

1. mnt namespace provides a root filesystem (this one can be compared to chroot I guess)
2. pid namespace so the process only sees itself and its children
3. network namespace which allows the container to have its dedicated network stack
4. user namespace (quite new) which allows a non root user on a host to be mapped with the root user within the container
5. uts provides dedicated hostname
6. ipc provides dedicated shared memory

All of this adds more isolation than chroot provides.

These extra bells and whistles is called process isolation, a container gets its own [namespace](https://en.wikipedia.org/wiki/Linux_namespaces) from the host kernel, that means the program in the container can't try to read kernel memory or eat more RAM than allowed.

It also isolates network stacks, so two process can listen on port 8080 for exemple, you'll have to handle the routing at host level, there's no magic here, but this allow handling the routing at one place and avoid modifying the process configuration to listen to a free port.

Secondly a chroot is still read/write, any change is permanent, a docker container using aufs will start from a clean filesystem each time you launch the container (changes are kept if you stop/start it IIRC).

So while a container may be thought of as process namespace + chroot, the reality is a little more complex.

## docker lingo

An **image** is a representation of everything you wish to have, but think of it as configuration. You’ve specified what you’d like to be in this image, and now, based on the image, you can create many containers.

A **container** is a running instance of a Docker image. Containers run the actual applications. A container includes an application and all of its dependencies. It shares the kernel with other containers and runs as an isolated process in user space on the host OS.

A **Docker daemon** is a background service running on the host that manages the building, running and distributing Docker containers.

**Docker client** is a command-line tool you use to interact with the Docker daemon. You call it by using the command docker on a terminal. You can use Kitematic to get a GUI version of the Docker client.

A **Docker store** is a registry of Docker images. There is a public registry on Docker.com where you can set up private registries for your team’s use. You can also easily create such a registry in Azure.

## docker

A Dockerfile is a text document that contains all the commands a user could call on the command line to assemble an image. Using docker build, users can create an automated build that executes several command-line instructions in succession. Docker builds images automatically by reading the instructions from a Dockerfile -- a text file that contains all commands, in order, needed to build a given image. A Docker image consists of read-only layers each of which represents a Dockerfile instruction. The layers are stacked and each one is a delta of the changes from the previous layer.

An example dockerfile would like the following:

```console
FROM ubuntu:18.04
COPY . /app
RUN make /app
CMD python /app/app.py
```

Each instruction creates one layer:

- FROM creates a layer from the ubuntu:18.04 Docker image.
- COPY adds files from your Docker client’s current directory.
- RUN builds your application with make.
- CMD specifies what command to run within the container.

When you run an image and generate a container, you add a new writable layer (the “container layer”) on top of the underlying layers. All changes made to the running container, such as writing new files, modifying existing files, and deleting files, are written to this thin writable container layer.

We will not use dockerfiles to

1. set up ubuntu based linux dev environment
2. add a user with sudo permissions
3. install vim, git etc.
4. install ohmyzsh
5. mount a volume for persistent data storage

```console
FROM ubuntu:latest
LABEL maintainer="aa <bb@cc.dd>"

RUN apt-get update && \
    apt-get install -y \
    sudo \
    curl \
    git-core \
    gnupg \
    linuxbrew-wrapper \
    locales \
    zsh \
    wget \
    vim \
    nano \
    npm \
    fonts-powerline && \
    locale-gen en_US.UTF-8 && \
    adduser --quiet --disabled-password --shell /bin/zsh --home /home/devuser --gecos "User" devuser && \
    echo "devuser:userpassword" | chpasswd &&  usermod -aG sudo devuser

USER devuser
ENV TERM xterm
CMD ["zsh"]
```

The first line says that your base image will be ubuntu:latest. Doing this tells Docker to use the Docker registry and find an image that matches this criterion. Specifically, the image you’ll use is this: https://hub.docker.com/_/ubuntu/.

The ADD command is used to copy files/directories into a Docker image. It can copy data in three ways:

- Copy files from the local storage to a destination in the Docker image.
- Copy a tarball from the local storage and extract it automatically inside a destination in the Docker image.
- Copy files from a URL to a destination inside the Docker image.

![add command](https://www.educative.io/api/edpresso/shot/6371088869097472/image/5249592310366208)

### steps

- Creating an image: In the same directory as where the Dockerfile resides, issue the following command:

```console
docker build --rm -f Dockerfile -t ubuntu:img .
```

Running this command creates the image for you.

- The Docker image is ready and when you run it, you are effectively creating a container. To run it, use:

```console
docker run --rm -it ubuntu:img
```

## useful commands

To remove all unused images, if you do not do this, you will run out of disk space

- sudo docker image prune
- sudo docker system prune -a

- docker image build: Build an image from a Dockerfile
- docker image history: Show the history of an image
- docker image import: Import the contents from a tarball to create a filesystem image
- docker image inspect: Display detailed information on one or more images
- docker image load: Load an image from a tar archive or STDIN
- docker image ls: List images
- docker image prune: Remove unused images
- docker image pull: Pull an image or a repository from a registry
- docker image push: Push an image or a repository to a registry
- docker image rm: Remove one or more images
- docker image save; Save one or more images to a tar archive (streamed to STDOUT by default)
- docker image tag: Create a tag TARGET_IMAGE that refers to SOURCE_IMAGE

## images and layers

By default all files created inside a container are stored on a writable container layer. This means that:

- The data doesn’t persist when that container no longer exists, and it can be difficult to get the data out of the container if another process needs it.
- A container’s writable layer is tightly coupled to the host machine where the container is running. You can’t easily move the data somewhere else.
- Writing into a container’s writable layer requires a storage driver to manage the filesystem. The storage driver provides a union filesystem, using the Linux kernel. This extra abstraction reduces performance as compared to using data volumes, which write directly to the host filesystem.

Docker has two options for containers to store files in the host machine for persistent storage, so that the files are persisted even after the container stops: _volumes_, and _bind mounts_.

Bind mounts exist on the host file system and being managed by the host maintainer.

With Volumes we can design our data effectively and decouple it from the host and other parts of the system by storing it dedicated remote locations (Cloud for example) and integrate it with external services like backups, monitoring, encryption and hardware management.

### Good use cases for volumes

Volumes are the preferred way to persist data in Docker containers and services. Some use cases for volumes include:

Sharing data among multiple running containers. If you don’t explicitly create it, a volume is created the first time it is mounted into a container. When that container stops or is removed, the volume still exists. Multiple containers can mount the same volume simultaneously, either read-write or read-only. Volumes are only removed when you explicitly remove them.

When the Docker host is not guaranteed to have a given directory or file structure. Volumes help you decouple the configuration of the Docker host from the container runtime.

When you want to store your container’s data on a remote host or a cloud provider, rather than locally.

When you need to back up, restore, or migrate data from one Docker host to another, volumes are a better choice. You can stop containers using the volume, then back up the volume’s directory (such as /var/lib/docker/volumes/<volume-name>).

### Good use cases for bind mounts

In general, you should use volumes where possible. Bind mounts are appropriate for the following types of use case:

Sharing configuration files from the host machine to containers. This is how Docker provides DNS resolution to containers by default, by mounting /etc/resolv.conf from the host machine into each container.

Sharing source code or build artifacts between a development environment on the Docker host and a container. For instance, you may mount a Maven target/ directory into a container, and each time you build the Maven project on the Docker host, the container gets access to the rebuilt artifacts.

If you use Docker for development this way, your production Dockerfile would copy the production-ready artifacts directly into the image, rather than relying on a bind mount.

When the file or directory structure of the Docker host is guaranteed to be consistent with the bind mounts the containers require.

## mounting a volume

```console
docker run --rm -it -v /home/user/Desktop/test:/datavol ubuntu:img
```

What we have done here is that we have mapped the host folder /home/user/Desktop/test to a volume /datavol that will be mounted inside our container.
